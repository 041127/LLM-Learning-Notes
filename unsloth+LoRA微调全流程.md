![image-20250712172554567](unsloth+LoRAå¾®è°ƒå…¨æµç¨‹.assets/image-20250712172554567.png)

[TOC]

# ä¸€ã€åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

## 1. Unslothä»‹ç»

`Unsloth` æ˜¯ä¸€ä¸ªå¼€æºå·¥å…·ï¼Œä¸“é—¨ç”¨æ¥åŠ é€Ÿå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å¾®è°ƒè¿‡ç¨‹ã€‚å®ƒçš„ä¸»è¦åŠŸèƒ½å’Œä¼˜åŠ¿åŒ…æ‹¬ï¼š

- é«˜æ•ˆå¾®è°ƒï¼šUnsloth çš„å¾®è°ƒé€Ÿåº¦æ¯”ä¼ ç»Ÿæ–¹æ³•å¿« 2-5 å€ï¼Œå†…å­˜å ç”¨å‡å°‘ 50%-80%ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ç”¨æ›´å°‘çš„èµ„æºå®Œæˆå¾®è°ƒä»»åŠ¡ã€‚
- ä½æ˜¾å­˜éœ€æ±‚ï¼šå³ä½¿æ˜¯æ¶ˆè´¹çº§ GPUï¼ˆå¦‚ RTX 3090ï¼‰ï¼Œä¹Ÿèƒ½è½»æ¾è¿è¡Œ Unslothã€‚ä¾‹å¦‚ï¼Œä»…éœ€ 7GB æ˜¾å­˜å°±å¯ä»¥è®­ç»ƒ 1.5B å‚æ•°çš„æ¨¡å‹ã€‚
- æ”¯æŒå¤šç§æ¨¡å‹å’Œé‡åŒ–ï¼šUnsloth æ”¯æŒ Llamaã€Mistralã€Phiã€Gemma ç­‰ä¸»æµæ¨¡å‹ï¼Œå¹¶ä¸”é€šè¿‡åŠ¨æ€ 4-bit é‡åŒ–æŠ€æœ¯ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨ï¼ŒåŒæ—¶å‡ ä¹ä¸æŸå¤±æ¨¡å‹ç²¾åº¦ã€‚
- å¼€æºä¸å…è´¹ï¼šUnsloth æä¾›å…è´¹çš„ Colab Notebookï¼Œç”¨æˆ·åªéœ€æ·»åŠ æ•°æ®é›†å¹¶è¿è¡Œä»£ç å³å¯å®Œæˆå¾®è°ƒã€‚

å®˜æ–¹æ–‡æ¡£ï¼š[[unslothai/unsloth: Fine-tuning & Reinforcement Learning for LLMs. ğŸ¦¥ Train Qwen3, Llama 4, DeepSeek-R1, Gemma 3, TTS 2x faster with 70% less VRAM.](https://github.com/unslothai/unsloth)](https://github.com/unslothai/unsloth)

```python
#å®‰è£…unsloth
conda create --name 1 \
    python=3.11 \
    pytorch-cuda=12.1 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate 1

pip install unsloth
#å®‰è£…vLLM&mistral common
pip install --upgrade vllm
pip install --upgrade mistral_common
```



```python
#åˆ©ç”¨unslothåŠ è½½æ¨¡å‹
from unsloth import FastLanguageModel, is_bfloat16_supported

max_seq_length=2048 #è®¾ç½®æ¨¡å‹å¤„ç†æ–‡æœ¬çš„æœ€å¤§é•¿åº¦
dtype=None #è®¾ç½®æ•°æ®ç±»å‹ï¼Œè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„ç²¾åº¦
load_in_4bit=True

model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="mistralai/Mistral-Nemo-Instruct-2407",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=load_in_4bit,#4bité‡åŒ–ï¼Œé€šè¿‡å‡å°‘æ¨¡å‹çš„ç²¾åº¦æ¥èŠ‚çœå†…å­˜ï¼Œä¸ºtrueæ˜¯qloraï¼Œfalseæ˜¯lora
    )
```

------



 ## 2. LoRA çš„åŸç†

å¯¹äºæŸä¸€çº¿æ€§å±‚æƒé‡çŸ©é˜µ $W \in \mathbb{R}^{d \times k}$ï¼ŒLoRA ä¸ç›´æ¥æ›´æ–° $W$ï¼Œè€Œæ˜¯åœ¨è®­ç»ƒæ—¶æ·»åŠ ä¸€ä¸ª **ä½ç§©çŸ©é˜µå˜åŒ–é¡¹**ï¼š

$$
W_{\text{new}} = W + \Delta W, \quad \Delta W = BA
$$
å…¶ä¸­ï¼š

- $A \in \mathbb{R}^{r \times k}$ï¼Œ$B \in \mathbb{R}^{d \times r}$ï¼Œ$r \ll \min(d,k)$ï¼›
- $A, B$ æ˜¯è®­ç»ƒå‚æ•°ï¼Œ$W$ å†»ç»“ä¸å˜ï¼›
- æ¨ç†æ—¶å¯å°† $W + BA$ åˆå¹¶ä¸ºå•ä¸€çŸ©é˜µï¼Œå‡ ä¹æ— æ¨ç†å¼€é”€ã€‚
- å…¨å‚æ•°æ›´æ–°éœ€è¦æ›´æ–°
    $$
    d*k
    $$
    ä¸ªå‚æ•°ï¼Œloraå¾®è°ƒåªéœ€è¦æ›´æ–°
    $$
    k*r+r*d=r*ï¼ˆk+dï¼‰
    $$
    ä¸ªå‚æ•°ï¼Œå¤§å¤§å‡å°‘äº†éœ€è¦æ›´æ–°çš„å‚æ•°

- è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒLoRA ä»…å¯¹å°‘é‡ä½ç§©çŸ©é˜µå‚æ•°è¿›è¡Œè°ƒæ•´ï¼Œç›¸æ¯”å…¨é‡å‚æ•°å¾®è°ƒï¼Œè®¡ç®—é‡å’Œå†…å­˜å ç”¨æ˜¾è‘—é™ä½ï¼Œè®­ç»ƒé€Ÿåº¦å¤§å¹…æå‡ï¼›åŒæ—¶ï¼Œå‚æ•°è°ƒæ•´é‡å°‘ï¼Œæå¤§é™ä½äº†è¿‡æ‹Ÿåˆé£é™©ï¼Œä½¿æ¨¡å‹æ³›åŒ–èƒ½åŠ›æ›´å¼ºã€‚åº”ç”¨å±‚é¢ï¼Œå®ƒèƒ½çµæ´»é€‚é…ä¸åŒä»»åŠ¡ï¼Œä¸ºåŒä¸€é¢„è®­ç»ƒæ¨¡å‹æ„å»ºå¤šä¸ª LoRA æ¨¡å—ï¼Œå¿«é€Ÿåˆ‡æ¢å®ç°å¤šä»»åŠ¡å­¦ä¹ ï¼›åœ¨éƒ¨ç½²é˜¶æ®µï¼Œå…¶æ³¨å…¥çš„çº¿æ€§å±‚å¯ä¸å†»ç»“å‚æ•°æ— ç¼åˆå¹¶ï¼Œä¸å¢åŠ æ¨ç†å»¶è¿Ÿã€‚

---

## 3. LoRA å®ç°ç»†èŠ‚

### æ’å…¥ LoRA å±‚

é€šå¸¸æ’å…¥ç‚¹æœ‰ï¼š

- Attentionï¼š
  - Queryï¼ˆQï¼‰å’Œ Valueï¼ˆVï¼‰æœ€å¸¸ç”¨ï¼›
- Linearï¼š
  - Feedforward ç½‘ç»œä¸­çš„ FC å±‚ï¼›

---

## 4. LoRAå¾®è°ƒå‚æ•°

```python
 # åº”ç”¨ LoRA å‚æ•°
    model = FastLanguageModel.get_peft_model(
        model,
        r=16,#ç§©
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj","gate_proj", "up_proj", "down_proj",],#åº”ç”¨loraæ–¹æ³•çš„æ¨¡å—åç§°
        lora_alpha=16,#ç¼©æ”¾ç³»æ•°ï¼Œä¸€èˆ¬æƒ…å†µä¸rç›¸åŒæˆ–ä¸ºrä¸¤å€
        lora_dropout=0.05,#å¤§æ•°æ®é›†ï¼š0.0ï¼Œå°æ•°æ®é›†ï¼š0.05-0.1
        bias="none",
        use_gradient_checkpointing="unsloth",
        #random_state = 3407,éšæœºæ•°ç§å­
        use_rslora=False,
        loftq_config=None,
    )
```

- LoRAï¼ˆä½ç§©é€‚åº”ï¼‰ä¸­çš„ç§©ï¼ˆRankï¼‰æ˜¯å†³å®šæ¨¡å‹å¾®è°ƒæ—¶å‚æ•°æ›´æ–° â€œè¡¨è¾¾èƒ½åŠ›â€ çš„å…³é”®å‚æ•°ã€‚å®ƒé€šè¿‡ä½ç§©çŸ©é˜µåˆ†è§£çš„æ–¹å¼ï¼Œæ§åˆ¶å¯è®­ç»ƒå‚æ•°çš„è§„æ¨¡ä¸æ¨¡å‹è°ƒæ•´çš„çµæ´»ç¨‹åº¦ã€‚ç§©çš„æ•°å€¼è¶Šå°ï¼Œæ¨¡å‹å¾®è°ƒæ—¶çš„å‚æ•°æ›´æ–°è¶Š â€œä¿å®ˆâ€ï¼›ç§©çš„æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½æ•æ‰çš„ç‰¹å¾å¤æ‚åº¦è¶Šé«˜ï¼Œä½†ä¹Ÿä¼šæ¶ˆè€—æ›´å¤šè®¡ç®—èµ„æºã€‚
- ç§©ä½ï¼ˆå¦‚ 4ï¼‰ï¼šç›¸å½“äºç”¨å›ºå®šçš„å‡ ç§ â€œæ€ç»´æ¨¡æ¿â€ å­¦ä¹ æ–°çŸ¥è¯†ã€‚æ¯”å¦‚å­¦æ•°å­¦æ—¶ï¼Œåªè®° 3 ç§è§£é¢˜å¥—è·¯ï¼Œé‡åˆ°æ–°é¢˜åªèƒ½ç”¨è¿™ 3 ç§æ–¹æ³•å¥—ã€‚ä¼˜ç‚¹æ˜¯å­¦ä¹ è¿‡ç¨‹å¾ˆç¨³å®šï¼Œä¸å®¹æ˜“å­¦åï¼ˆè¿‡æ‹Ÿåˆé£é™©ä½ï¼‰ï¼Œä¸” â€œè„‘å­â€ï¼ˆæ˜¾å­˜ï¼‰ä¸è´¹åŠ›ï¼›ç¼ºç‚¹æ˜¯é‡åˆ°å¤æ‚é—®é¢˜å¯èƒ½æŸæ‰‹æ— ç­–ï¼Œå› ä¸ºæ¨¡æ¿å¤ªå°‘ï¼ˆè¡¨è¾¾èƒ½åŠ›æœ‰é™ï¼‰ã€‚
- ç§©é«˜ï¼ˆå¦‚ 64ï¼‰ï¼šç›¸å½“äºæŒæ¡äº† 100 ç§è§£é¢˜æ€è·¯ï¼Œé‡åˆ°æ–°é¢˜å¯ä»¥çµæ´»ç»„åˆæ–¹æ³•ã€‚ä¼˜ç‚¹æ˜¯èƒ½å¤„ç†æ›´å¤æ‚çš„ä»»åŠ¡ï¼ˆæ¯”å¦‚ç”Ÿæˆé£æ ¼æ›´ç»†è…»çš„å†…å®¹ï¼‰ï¼Œæ¨¡å‹å¾®è°ƒçš„ â€œæ½œåŠ›â€ æ›´å¤§ï¼›ç¼ºç‚¹æ˜¯å®¹æ˜“å­¦ â€œä¹±â€â€”â€” å¯èƒ½æŠŠæ— å…³çš„çŸ¥è¯†å¼ºè¡Œå…³è”ï¼ˆè¿‡æ‹Ÿåˆï¼‰ï¼Œè€Œä¸”å¤ªè´¹ â€œè„‘å­â€ï¼ˆæ˜¾å­˜æ¶ˆè€—æ˜¾è‘—å¢åŠ ï¼‰ã€‚
- æ—¥å¸¸å¾®è°ƒå»ºè®®ä» 8-16 å¼€å§‹å°è¯•ï¼Œè¿™æ˜¯å¹³è¡¡æ•ˆæœä¸æ•ˆç‡çš„å¸¸ç”¨åŒºé—´ã€‚ä¸€èˆ¬å°±æ˜¯ä» 8 å¼€å§‹ï¼Œå¦‚æœå¾®è°ƒå®Œè§‰å¾—æ¨¡å‹æ²¡å­¦ä¼šå°±è°ƒå¤§ã€‚

![image-20250710112907051](unsloth+LoRAå¾®è°ƒå…¨æµç¨‹.assets/image-20250710112907051.png)

---

## åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å®Œæ•´ä»£ç 

```python
#åˆ©ç”¨unslothåŠ è½½æ¨¡å‹
from unsloth import FastLanguageModel, is_bfloat16_supported

max_seq_length=2048 #è®¾ç½®æ¨¡å‹å¤„ç†æ–‡æœ¬çš„æœ€å¤§é•¿åº¦
dtype=None #è®¾ç½®æ•°æ®ç±»å‹ï¼Œè®©æ¨¡å‹è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„ç²¾åº¦

model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="mistralai/Mistral-Nemo-Instruct-2407",
        max_seq_length=max_seq_length,
        dtype=dtype,
        load_in_4bit=True,#4bité‡åŒ–ï¼Œé€šè¿‡å‡å°‘æ¨¡å‹çš„ç²¾åº¦æ¥èŠ‚çœå†…å­˜ï¼Œä¸ºtrueæ˜¯qloraï¼Œfalseæ˜¯lora
    )

# åº”ç”¨ LoRA å‚æ•°
model = FastLanguageModel.get_peft_model(
    model,
    r=32,#ç§©
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "embed_tokens", "lm_head"],#åº”ç”¨loraæ–¹æ³•çš„æ¨¡å—åç§°
    lora_alpha=64,#ç¼©æ”¾ç³»æ•°ï¼Œä¸€èˆ¬æƒ…å†µä¸ºrä¸¤å€
    lora_dropout=0.05,#å¤§æ•°æ®é›†ï¼š0.0ï¼Œå°æ•°æ®é›†ï¼š0.05-0.1
    bias="none",
    use_gradient_checkpointing="unsloth",
    use_rslora=False,
    loftq_config=None,
)
model.model.model.embed_tokens = model.model.model.embed_tokens.to(dtype=torch.bfloat16)
model.model.lm_head = model.model.lm_head.to(dtype=torch.bfloat16)

```

# äºŒã€å¾®è°ƒå‰æµ‹è¯•

è®¾ç½®æ¨ç†é—®é¢˜ï¼ˆæŒ‰ç…§ä½ æƒ³è¦å¾®è°ƒçš„æ–¹å‘è®¾ç½®ï¼‰

```python
prompt_style = """ä»¥ä¸‹æ˜¯æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚è¯·å†™å‡ºä¸€ä¸ªé€‚å½“å®Œæˆè¯·æ±‚çš„å›ç­”ã€‚
åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒé—®é¢˜ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªé€»è¾‘è¿è´¯çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›ç­”å‡†ç¡®æ— è¯¯ã€‚

### æŒ‡ä»¤ï¼š
ä½ æ˜¯ä¸€ä½ç²¾é€šåœå¦ã€æ˜Ÿè±¡å’Œè¿åŠ¿é¢„æµ‹çš„ç®—å‘½å¤§å¸ˆã€‚
è¯·å›ç­”ä»¥ä¸‹ç®—å‘½é—®é¢˜ã€‚

### é—®é¢˜ï¼š
{}

### å›ç­”ï¼š
<think>{}</think>""" 
# å®šä¹‰æç¤ºé£æ ¼çš„å­—ç¬¦ä¸²æ¨¡æ¿ï¼Œç”¨äºæ ¼å¼åŒ–é—®é¢˜

question = "1992å¹´é—°å››æœˆåˆä¹å·³æ—¶ç”Ÿäººï¼Œå¥³ï¼Œæƒ³äº†è§£å¥åº·è¿åŠ¿" 
# å®šä¹‰å…·ä½“çš„ç®—å‘½é—®é¢˜
```

```python
FastLanguageModel.for_inference(model)
# å‡†å¤‡æ¨¡å‹ä»¥è¿›è¡Œæ¨ç†

inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")
# ä½¿ç”¨ tokenizer å¯¹æ ¼å¼åŒ–åçš„é—®é¢˜è¿›è¡Œç¼–ç ï¼Œå¹¶ç§»åŠ¨åˆ° GPU

outputs = model.generate(
    input_ids=inputs.input_ids,
    attention_mask=inputs.attention_mask,
    max_new_tokens=1200,
    use_cache=True,
)
# ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›ç­”

response = tokenizer.batch_decode(outputs)
# è§£ç æ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºä¸ºå¯è¯»æ–‡æœ¬

print(response[0])
# æ‰“å°ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†
```



# ä¸‰ã€åŠ è½½æ•°æ®é›†

## 1. ä¿®æ”¹æ•°æ®é›†çš„æ ¼å¼ä¸ºchatml

åœ¨è¿™ä¸€æ­¥é‡Œï¼Œæ¯ä¸€ä¸ªæ•°æ®æ®µæœ«å°¾åŠ ä¸ŠEOS_TOKEN 

EOS_TOKEN = tokenizer.eos_token

ä»¥alpacaä¸ºä¾‹

```python
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass

from datasets import load_dataset
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)
```



```python
import json
import re
from tqdm import tqdm
from datasets import load_dataset, Dataset

def chatml_turn(role: str, content: str) -> str:
    return f"<|im_start|>{role}\n{content}\n<|im_end|>"

def extract_user_assistant_turns(text):
    """
    ä»åŸå§‹å­—ç¬¦ä¸²ä¸­æå–å¤šè½® <|user|>...<|assistant|> å¯¹è¯ã€‚
    è¿”å› ChatML æ ¼å¼çš„è½®æ¬¡åˆ—è¡¨ã€‚
    """
    turns = re.findall(r"<\|user\|>(.*?)<\|assistant\|>(.*?)(?=(<\|user\|>|$))", text, re.DOTALL)
    formatted = []
    for user_text, assistant_text, _ in turns:
        user_text = user_text.strip()
        assistant_text = assistant_text.strip()
        if user_text and assistant_text:
            formatted.append(chatml_turn("user", user_text))
            formatted.append(chatml_turn("assistant", assistant_text))
    return formatted

def process_roleplay_dataset():
    """
    å¤„ç† hieunguyenminh/roleplay æ•°æ®é›†
    """
    dataset = load_dataset("hieunguyenminh/roleplay", split="train")
    formatted = []

    for example in tqdm(dataset, desc="Formatting hieunguyenminh/roleplay"):
        description = example.get("description", "").strip()
        text = example.get("text", "").strip()
        if not description or not text:
            continue

        header = chatml_turn("system", description)
        dialogue_turns = extract_user_assistant_turns(text)
        if not dialogue_turns:
            continue

        full_text = header + "\n" + "\n".join(dialogue_turns)
        formatted.append({"text": full_text})

    return formatted

def process_charcard_dataset():
    """
    å¤„ç† Gryphe/Sonnet3.5-Charcard-Roleplay æ•°æ®é›†
    """
    dataset = load_dataset("Gryphe/Sonnet3.5-Charcard-Roleplay", split="train")
    formatted = []

    for example in tqdm(dataset, desc="Formatting Gryphe/Charcard"):
        conversations = example.get("conversations", [])
        if not conversations or conversations[0]["from"] != "system":
            continue
        if not any(msg["from"] == "gpt" for msg in conversations):
            continue  # å¿…é¡»è‡³å°‘æœ‰ assistant å›å¤

        turns = [chatml_turn("system", conversations[0]["value"].strip())]

        for msg in conversations[1:]:
            role = msg["from"]
            content = msg["value"].strip()
            if role == "human":
                turns.append(chatml_turn("user", content))
            elif role == "gpt":
                turns.append(chatml_turn("assistant", content))

        full_text = "\n".join(turns)
        formatted.append({"text": full_text})

    return formatted

def process_feedback_dataset():
    """
    å¤„ç† rica40325/9.8feedback æ•°æ®é›†
    """
    dataset = load_dataset("rica40325/9.8feedback", split="train")
    formatted = []

    for example in tqdm(dataset, desc="Formatting rica40325/9.8feedback"):
        conversations = example.get("conversations", [])
        if not conversations or conversations[0]["from"] != "system":
            continue
        if not any(msg["from"] == "assistant" for msg in conversations):
            continue  # å¿…é¡»è‡³å°‘æœ‰ assistant å›å¤

        # ç¬¬ä¸€æ¡ system æ¶ˆæ¯
        turns = [chatml_turn("system", conversations[0]["value"].strip())]

        # å¤„ç†åç»­è½®æ¬¡
        for msg in conversations[1:]:
            role = msg["from"]
            content = msg["value"].strip()
            if role == "user":
                turns.append(chatml_turn("user", content))
            elif role == "assistant":
                turns.append(chatml_turn("assistant", content))

        # ä¸€æ•´æ¡å¯¹è¯ä¿å­˜ä¸€æ¬¡
        full_text = "\n".join(turns)
        formatted.append({"text": full_text})

    return formatted

def is_valid_chatml(text):
    """ç¡®ä¿æ¯ä¸ªæ ·æœ¬éƒ½è‡³å°‘åŒ…å« user å’Œ assistant ä¸¤ç§è§’è‰²"""
    return "<|im_start|>user" in text and "<|im_start|>assistant" in text

def save_as_jsonl(data, path):
    with open(path, "w", encoding="utf-8") as f:
        for item in data:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

if __name__ == "__main__":
    print(" æ­£åœ¨æ ¼å¼åŒ–ä¸‰ä¸ªè§’è‰²æ‰®æ¼”æ•°æ®é›†...")

    data1 = process_roleplay_dataset()
    data2 = process_charcard_dataset()
    data3 = process_feedback_dataset()
    all_data = data1 + data2 + data3

    print(f" æ ¼å¼åŒ–å‰æ ·æœ¬æ€»æ•°: {len(all_data)}")

    # æ¸…æ´—ä¸ç¬¦åˆ ChatML çš„æ ·æœ¬
    cleaned_data = [item for item in all_data if is_valid_chatml(item["text"])]
    print(f" æ ¼å¼åŒ–ååˆæ³•æ ·æœ¬æ•°: {len(cleaned_data)}")

    # ä¿å­˜ä¸º jsonl
    save_path = "/home/qwen-sft/dataset-sft/processed_data.jsonl"
    save_as_jsonl(cleaned_data, save_path)
    print(f" å·²ä¿å­˜åˆ°: {save_path}")

```

## 2. å°†å¤„ç†å¥½æ ¼å¼çš„æ•°æ®è¿›è¡Œtokenize

```python
# åˆ¤æ–­æ˜¯å¦å·²æœ‰ç¼“å­˜çš„ Tokenized æ•°æ®é›†
if os.path.exists(TOKENIZED_PATH):
    print("åŠ è½½ç¼“å­˜çš„ tokenized æ•°æ®é›†...")
    tokenized_dataset = load_from_disk(TOKENIZED_PATH)
else:
    print("é¦–æ¬¡å¤„ç†æ•°æ®å¹¶ä¿å­˜ç¼“å­˜...")
    raw_dataset = load_dataset("json", data_files="/home/qwen-sft/dataset-sft/processed_data.jsonl", split="train")
    dataset = raw_dataset.filter(lambda x: isinstance(x["text"], str) and len(x["text"].strip()) > 50)

    # Tokenization
    def tokenize(example):
        return tokenizer(example["text"], truncation=True, max_length=2048, padding="max_length")

    tokenized_dataset = dataset.map(tokenize, batched=True, num_proc=14)

    # ä¿å­˜æˆ Arrow æ ¼å¼
    tokenized_dataset.save_to_disk(TOKENIZED_PATH)
    print(f" Tokenized æ•°æ®é›†å·²ä¿å­˜åˆ° {TOKENIZED_PATH}")
```



# å››ã€è®­ç»ƒè¶…å‚æ•°è®¾ç½®

**è®­ç»ƒè½®æ•°ï¼ˆNumber of Epochsï¼‰** Epoch æ˜¯æœºå™¨å­¦ä¹ ä¸­ç”¨äºæè¿°æ¨¡å‹è®­ç»ƒè¿‡ç¨‹çš„ä¸€ä¸ªæœ¯è¯­ï¼ŒæŒ‡çš„æ˜¯æ¨¡å‹å®Œæ•´åœ°éå†ä¸€æ¬¡æ•´ä¸ªè®­ç»ƒæ•°æ®é›†çš„æ¬¡æ•°ã€‚æ¢å¥è¯è¯´ï¼Œä¸€ä¸ª `Epoch` è¡¨ç¤ºæ¨¡å‹å·²ç»çœ‹åˆ°äº†æ‰€æœ‰è®­ç»ƒæ ·æœ¬ä¸€æ¬¡ã€‚

- è½®æ•°å°‘ï¼šæ¯”å¦‚ä½ åªå¤ä¹ ä¸€éï¼Œå¯èƒ½å¯¹ä¹¦é‡Œçš„å†…å®¹è¿˜ä¸æ˜¯å¾ˆç†Ÿæ‚‰ï¼Œè€ƒè¯•æˆç»©å¯èƒ½ä¸ä¼šå¤ªç†æƒ³ã€‚
- è½®æ•°å¤šï¼šæ¯”å¦‚ä½ å¤ä¹ äº† 10 éï¼Œå¯¹ä¹¦é‡Œçš„å†…å®¹å°±å¾ˆç†Ÿæ‚‰äº†ï¼Œä½†å¯èƒ½ä¼šå‡ºç°ä¸€ä¸ªé—®é¢˜â€”â€”ä½ å¯¹ä¹¦é‡Œçš„å†…å®¹èƒŒå¾—å¾ˆç†Ÿï¼Œä½†é‡åˆ°æ–°çš„ã€ç±»ä¼¼çš„é—®é¢˜å°±ä¸ä¼šè§£ç­”äº†ï¼Œç®€å•è®²å°±æ˜¯ â€œå­¦å‚»äº†â€œï¼Œåªè®°ä½è¿™æœ¬ä¹¦é‡Œçš„å†…å®¹äº†ï¼Œç¨å¾®å˜ä¸€å˜å°±ä¸ä¼šäº†ï¼ˆ**è¿‡æ‹Ÿåˆ**ï¼‰ã€‚
- ä¸€èˆ¬æƒ…å†µä¸‹ 3 è½®å°±å¯ä»¥ï¼Œåœ¨å®é™…å¼€å§‹è®­ç»ƒåï¼Œåªè¦ LOSS æ²¡æœ‰è¶‹äºå¹³ç¼“ï¼Œå°±å¯ä»¥å†ç»§ç»­åŠ å¤§è®­ç»ƒè½®æ•°ï¼Œåä¹‹å¦‚æœ LOSS å¼€å§‹æå‰æ”¶æ•›ï¼Œå°±å¯ä»¥å‡å° Epoch æˆ–è€…æå‰ç»“æŸã€‚æ³¨æ„ä¸è¦æŠŠ LOSS é™çš„è¿‡ä½ï¼ˆè¶‹è¿‘äºé›¶ï¼‰ï¼Œè®­ç»ƒè½®æ•°å°½é‡ä¸è¦è¶…è¿‡ 10 ï¼Œè¿™ä¸¤ç§æƒ…å†µå¤§æ¦‚ç‡ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆæŠŠæ¨¡å‹ç»ƒå‚»ï¼‰ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæ•°æ®é›†è¶Šå°ï¼Œéœ€è¶Šå¤š Epochï¼›æ•°æ®é›†è¶Šå¤§ï¼Œéœ€è¶Šå°‘ Epochï¼ŒLOSS æ§åˆ¶åœ¨ 0.5 -> 1.5 ä¹‹é—´ã€‚  

**å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰** å†³å®šäº†æ¨¡å‹åœ¨æ¯æ¬¡æ›´æ–°æ—¶å‚æ•°è°ƒæ•´çš„å¹…åº¦ï¼Œé€šå¸¸åœ¨ (0, 1) ä¹‹é—´ã€‚ä¹Ÿå°±æ˜¯å‘Šè¯‰æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ â€œå­¦ä¹ â€ çš„é€Ÿåº¦æœ‰å¤šå¿«ã€‚å­¦ä¹ ç‡è¶Šå¤§ï¼Œæ¨¡å‹æ¯æ¬¡è°ƒæ•´çš„å¹…åº¦å°±è¶Šå¤§ï¼›å­¦ä¹ ç‡è¶Šå°ï¼Œè°ƒæ•´çš„å¹…åº¦å°±è¶Šå°ã€‚

- å­¦ä¹ ç‡å¤§ï¼ˆæ¯”å¦‚0.1ï¼‰ï¼šæ¯æ¬¡åšå®Œä¸€é“é¢˜åï¼Œä½ ä¼šå¯¹è§£é¢˜æ–¹æ³•è¿›è¡Œå¾ˆå¤§çš„è°ƒæ•´ã€‚æ¯”å¦‚ï¼Œä½ å¯èƒ½ä¼šå®Œå…¨æ”¹å˜è§£é¢˜æ€è·¯ã€‚ä¼˜ç‚¹æ˜¯è¿›æ­¥å¯èƒ½å¾ˆå¿«ï¼Œå› ä¸ºä½ æ¯æ¬¡éƒ½åœ¨è¿›è¡Œè¾ƒå¤§çš„è°ƒæ•´ã€‚ç¼ºç‚¹å°±æ˜¯å¯èƒ½ä¼šå› ä¸ºè°ƒæ•´å¹…åº¦è¿‡å¤§è€Œâ€œèµ°åâ€ï¼Œæ¯”å¦‚çªç„¶æ”¹å˜äº†ä¸€ä¸ªå·²ç»æŒæ¡å¾—å¾ˆå¥½çš„æ–¹æ³•ï¼Œå¯¼è‡´ä¹‹å‰å­¦çš„ä¸œè¥¿éƒ½å¿˜äº†ã€‚
- å­¦ä¹ ç‡å°ï¼ˆæ¯”å¦‚0.0001ï¼‰ï¼šæ¯æ¬¡åšå®Œä¸€é“é¢˜åï¼Œä½ åªå¯¹è§£é¢˜æ–¹æ³•è¿›è¡Œéå¸¸ç»†å¾®çš„è°ƒæ•´ã€‚æ¯”å¦‚ï¼Œä½ å‘ç°æŸä¸ªæ­¥éª¤æœ‰ç‚¹å°é”™è¯¯ï¼Œå°±åªè°ƒæ•´é‚£ä¸ªå°é”™è¯¯ã€‚ä¼˜ç‚¹æ˜¯**éå¸¸ç¨³å®š**ï¼Œä¸ä¼šå› ä¸ºä¸€æ¬¡é”™è¯¯è€Œâ€œèµ°åâ€ï¼Œé€‚åˆéœ€è¦ç²¾ç»†è°ƒæ•´çš„åœºæ™¯ã€‚ç¼ºç‚¹å°±æ˜¯**è¿›æ­¥ä¼šå¾ˆæ…¢**ï¼Œå› ä¸ºä½ æ¯æ¬¡åªè°ƒæ•´ä¸€ç‚¹ç‚¹ã€‚
- ä¸€èˆ¬ä¿æŒåœ¨ 5e-5ï¼ˆ0.00005ï¼‰ å’Œ 4e-5ï¼ˆ0.00004ï¼‰ä¹‹é—´ï¼Œå°æ•°æ®é›†å°½é‡ä¸è¦ç”¨å¤§å­¦ä¹ ç‡ï¼Œä¸è¦æ‹…å¿ƒé€Ÿåº¦æ…¢ï¼Œé€Ÿåº¦å¿«äº†åè€Œä¼šå½±å“å¾®è°ƒæ•ˆæœã€‚

**æ‰¹é‡å¤§å°ï¼ˆBatch Sizeï¼‰** æ˜¯æŒ‡åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°æ—¶æ‰€ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ã€‚å®ƒæ˜¯è®­ç»ƒæ•°æ®è¢«åˆ†å‰²æˆçš„å°å—ï¼Œæ¨¡å‹æ¯æ¬¡å¤„ç†ä¸€ä¸ªå°å—çš„æ•°æ®æ¥æ›´æ–°å‚æ•°ã€‚

é€šä¿—æ¥è¯´ï¼Œæ‰¹é‡å¤§å°å¯ä»¥ç”¨æ¥å¹³è¡¡å¤ä¹ é€Ÿåº¦å’Œä¸“æ³¨åº¦ï¼Œç¡®ä¿æ—¢èƒ½å¿«é€Ÿæ¨è¿›å¤ä¹ è¿›åº¦ï¼Œåˆèƒ½ä¸“æ³¨ç»†èŠ‚ã€‚å‡è®¾ä½ å†³å®šæ¯æ¬¡å¤ä¹ æ—¶é›†ä¸­ç²¾åŠ›åšä¸€å®šæ•°é‡çš„é¢˜ç›®ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡åªåšä¸€é“é¢˜ã€‚

- æ‰¹é‡å¤§ï¼ˆæ¯”å¦‚100ï¼‰ï¼šæ¯æ¬¡å¤ä¹ æ—¶ï¼Œä½ é›†ä¸­ç²¾åŠ›åš100é“é¢˜ã€‚**ä¼˜ç‚¹æ˜¯å¤ä¹ é€Ÿåº¦å¾ˆå¿«ï¼Œå› ä¸ºä½ æ¯æ¬¡å¤„ç†å¾ˆå¤šé¢˜ç›®ï¼Œèƒ½å¿«é€Ÿäº†è§£æ•´ä½“æƒ…å†µï¼ˆè®­ç»ƒæ›´ç¨³å®šï¼Œæ˜“æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜ï¼‰**ã€‚**ç¼ºç‚¹æ˜¯å¯èƒ½ä¼šå› ä¸ºä¸€æ¬¡å¤„ç†å¤ªå¤šé¢˜ç›®è€Œæ„Ÿåˆ°å‹åŠ›è¿‡å¤§ï¼Œç”šè‡³é”™è¿‡ä¸€äº›ç»†èŠ‚ï¼ˆè€—æ˜¾å­˜ï¼Œæ³›åŒ–èƒ½åŠ›å·®ï¼Œæ˜“è¿‡æ‹Ÿåˆï¼‰ã€‚**
- æ‰¹é‡å°ï¼ˆæ¯”å¦‚1ï¼‰ï¼šæ¯æ¬¡å¤ä¹ æ—¶ï¼Œä½ åªåšä¸€é“é¢˜ï¼Œåšå®Œåå†åšä¸‹ä¸€é“ã€‚**ä¼˜ç‚¹æ˜¯å¯ä»¥éå¸¸ä¸“æ³¨ï¼Œèƒ½ä»”ç»†åˆ†ææ¯é“é¢˜çš„ç»†èŠ‚ï¼Œé€‚åˆéœ€è¦æ·±å…¥ç†è§£çš„åœºæ™¯ï¼ˆçœæ˜¾å­˜ï¼Œæ˜“æ•æ‰æ•°æ®ç»†èŠ‚ï¼Œæ³›åŒ–èƒ½åŠ›å¼ºï¼‰ã€‚****ç¼ºç‚¹å°±æ˜¯å¤ä¹ é€Ÿåº¦å¾ˆæ…¢ï¼Œå› ä¸ºæ¯æ¬¡åªå¤„ç†ä¸€é“é¢˜ï¼ˆè®­ç»ƒä¸ç¨³å®šï¼Œæ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼‰ã€‚**
- ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¤§ batch_size éœ€æ­é…å¤§å­¦ä¹ ç‡ï¼Œå° batch_size éœ€æ­é…å°å­¦ä¹ ç‡ï¼Œå¯¹äºä¸€äº›å°å‚æ•°æ¨¡å‹ã€å°æ•°æ®é›†çš„å¾®è°ƒï¼Œå• GPU æ‰¹å¤„ç†å¤§å°ï¼ˆPer Device Train Batch Sizeï¼‰å»ºè®®ä» 2 ç”šè‡³æ˜¯ 1 å¼€å§‹ï¼ˆè¦æƒ³çœæ˜¾å­˜å°±ç›´æ¥è®¾ç½® 1ï¼‰ï¼Œé€šè¿‡è°ƒå¤§æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¯”å¦‚è®¾ç½®ä¸º 4 æˆ–è€… 8ï¼‰æ¥åŠ å¿«æ¨¡å‹å¾®è°ƒé€Ÿåº¦ã€‚

### è¶…å‚æ•°è®¾ç½®å…·ä½“ä»£ç 

```python
from unsloth import is_bfloat16_supported
from transformers import TrainingArguments
from trl import SFTTrainer, DataCollatorForCompletionOnlyLM

# è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir="/home/qwen-sft/model-sft/output",
        per_device_train_batch_size=16,
        gradient_accumulation_steps=1,
        save_steps=500,
        num_train_epochs=3,
        learning_rate=2e-5,
        fp16=not is_bfloat16_supported(),
        bf16=is_bfloat16_supported(),
        logging_steps=10,
        optim="paged_adamw_8bit",#ä½¿ç”¨çš„ä¼˜åŒ–å™¨ï¼Œç”¨æ¥è°ƒæ•´æ¨¡å‹å‚æ•°
        lr_scheduler_type="cosine",#å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹ï¼Œæ§åˆ¶å­¦ä¹ ç‡çš„å˜åŒ–æ–¹å¼
        warmup_ratio=0.03,
        save_total_limit=2,
        report_to="wandb",
        run_name="qwen-sft-run1",
        max_grad_norm=1.0
    )
    
data_collator = DataCollatorForCompletionOnlyLM(
    response_template="<|im_start|>assistant",
    instruction_template="<|im_start|>user",
    tokenizer=tokenizer,
)
# SFT Trainer
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=tokenized_dataset,
        dataset_text_field="text",#æŒ‡å®šæ•°æ®é›†ä¸­æ–‡æœ¬å­—æ®µçš„åç§°
        max_seq_length=max_seq_length,
        args=training_args,
        packing=False,#æ˜¯å¦å¯ç”¨æ‰“åŒ…åŠŸèƒ½ï¼Œæ‰“åŒ…å¯ä»¥è®©è®­ç»ƒæ›´å¿«ï¼Œä½†å¯èƒ½å½±å“æ•ˆæœ
        data_collator=data_collator,
        #dataset_num_proc=2#è®¾ç½®æ•°æ®å¤„ç†çš„å¹¶è¡Œè¿›ç¨‹æ•°
    )


```



# äº”ã€åˆå¹¶æ¨¡å‹ï¼ˆä»£ç ï¼‰

```python
# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False:
    model.save_pretrained("model")
    tokenizer.save_pretrained("model")
if False:
    model.push_to_hub("hf/model", token = "")
    tokenizer.push_to_hub("hf/model", token = "")
```

```python
# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
# Remember to go to https://huggingface.co/settings/tokens for a token!
# And change hf to your username!
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

# Save to multiple GGUF options - much faster if you want multiple!
if False:
    model.push_to_hub_gguf(
        "hf/model", # Change hf to your username!
        tokenizer,
        quantization_method = ["q4_k_m", "q8_0", "q5_k_m",],
        token = "", # Get a token at https://huggingface.co/settings/tokens
    )
```



```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# Step 1: åŠ è½½åŸå§‹æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Step 2: åŠ è½½è®­ç»ƒå¥½çš„ LoRA adapte
peft_model = PeftModel.from_pretrained(base_model, "/home/qwen-sft/model-sft/output/checkpoint-4000")

# Step 3: åˆå¹¶æƒé‡
merged_model = peft_model.merge_and_unload()

# Step 4: ä¿å­˜åˆå¹¶åçš„æ¨¡å‹ï¼ˆå¯ä¸Šä¼  HFï¼‰
merged_model.save_pretrained("/home/qwen-sft/model-sft/merged")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")
tokenizer.save_pretrained("/home/qwen-sft/model-sft/merged")

```

#  å…­ã€æµ‹è¯•æ¨¡å‹

```python
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)
```

- äº¤äº’å¼å¯¹è¯æµ‹è¯•

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# åŠ è½½æ¨¡å‹ï¼ˆä½ å·²è®­ç»ƒå¥½çš„ï¼‰
model_name = "hahayang012/Qwen3-8B-SFT"

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)
model.eval()

# åˆå§‹åŒ–å¯¹è¯å†å²
history = [
    {"role": "system", "content": ""},##è¿™é‡Œè¾“å…¥system prompt å’Œå†å²å¯¹è¯
    {"role": "user", "content": ""},
    {"role": "assistant", "content": ""},
    {"role": "user", "content": ""},
    {"role": "assistant", "content": ""}
]

# æ„å»º ChatML Prompt
def build_chatml_prompt(history):
    prompt = ""
    for turn in history:
        prompt += f"<|im_start|>{turn['role']}\n{turn['content']}\n<|im_end|>\n"
    prompt += "<|im_start|>assistant\n"  # ç•™ç»™æ¨¡å‹ç”Ÿæˆ
    return prompt

# å¯¹è¯å‡½æ•°
def chat():
    print("å¼€å§‹å¯¹è¯å§ï¼ï¼ˆè¾“å…¥ exit æˆ– é€€å‡º æ¥ç»“æŸï¼‰")
    while True:
        user_input = input("ä½ ï¼š")
        if user_input.lower() in ["exit", "é€€å‡º", "quit"]:
            print("å·²ç»“æŸå¯¹è¯ã€‚")
            break

        history.append({"role": "user", "content": user_input})
        prompt = build_chatml_prompt(history)

        # ç¼–ç  + ç”Ÿæˆ
        inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=False).to("cuda")
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=1024,
            do_sample=True,
            temperature=0.95,
            top_k=20,
            top_p=0.85,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
        )

        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
        response = decoded.split("<|im_start|>assistant\n")[-1].split("<|im_end|>")[0].strip()
        history.append({"role": "assistant", "content": response})
        print(f"assistantï¼š{response}")

# å¯åŠ¨å¯¹è¯
chat()

```

# ä¸ƒã€æ‰“æ¦œè¯„æµ‹å‚æ•°

| å‚æ•°å                 | æ¨èå€¼  | ç”¨æ¥å¹²å˜›ï¼ˆé€šä¿—è§£é‡Šï¼‰                                         |
| ---------------------- | ------- | ------------------------------------------------------------ |
| **Top K**              | 50      | é™åˆ¶â€œä¸‹ä¸€ä¸ªè¯â€çš„å€™é€‰è¯æ•°é‡ã€‚ä¾‹å¦‚ï¼šæ¨¡å‹åŸæœ¬èƒ½é€‰10000ä¸ªè¯ï¼Œä½†åªè€ƒè™‘æ¦‚ç‡æœ€é«˜çš„50ä¸ªè¯ã€‚é˜²æ­¢é€‰åˆ°å¥‡æ€ªè¯ã€‚ |
| **Top P**              | 0.9     | å†åŠ ä¸€å±‚â€œæ€»æ¦‚ç‡è¿‡æ»¤â€ã€‚ä»æ¦‚ç‡é«˜åˆ°ä½é€‰å‡ºä¸€ç»„è¯ï¼Œç›´åˆ°åŠ èµ·æ¥è¶…è¿‡90%æ¦‚ç‡ã€‚åŠ¨æ€æ§åˆ¶å€™é€‰è¯ï¼Œç¨³ä¸­æ±‚å˜ã€‚ |
| **Min P**              | 0.01    | è®¾å®šæœ€ä½â€œå…¥é€‰é—¨æ§›â€ï¼Œå¤ªå†·é—¨çš„è¯ä¸€å¾‹ä¸è€ƒè™‘ã€‚è¿™ä¸ªå€¼è¶Šé«˜ï¼Œæ¨¡å‹è¶Šä¿å®ˆã€‚ä¸€èˆ¬ä¸ç”¨è°ƒã€‚ |
| **Temperature**        | 0.7â€“0.9 | æ§åˆ¶â€œé€‰è¯çš„æ¿€è¿›ç¨‹åº¦â€ã€‚å€¼è¶Šä½è¶Šç¨³é‡ï¼ˆæ›´å‡†ç¡®ï¼‰ï¼Œå€¼è¶Šé«˜è¶Šå‘æ•£ï¼ˆæ›´æœ‰åˆ›æ„ï¼‰ã€‚ |
| **Repetition Penalty** | 1.1â€“1.2 | æƒ©ç½šæ¨¡å‹é‡å¤ç”¨è¯çš„è¡Œä¸ºã€‚æ•°å€¼è¶Šé«˜ï¼Œè¶Šä¸å®¹æ˜“ç”Ÿæˆé‡å¤å†…å®¹ã€‚é˜²æ­¢å‡ºç°â€œå•°å—¦â€æˆ–â€œç­”å¤ä¸€æ¨¡ä¸€æ ·â€çš„æƒ…å†µã€‚ |
