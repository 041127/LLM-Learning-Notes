### 需要的库

NumPy 、pandas 、IPython和Jupyter 、SciPy 、scikit-learn、statsmodels



## 构建词表

1. 构建text、label列表
2. 用os库获取文件路径和路径下所有的子目录、文件
3. 从文件中读取出text和label并通过append加入到列表中 
4. 小写化 text.lower()
5. 去除符号
6. 分词spilt
7. 取出现数量最多的



## **包装数据集**dataset/dataloader

Dataset：__init__初始化 __len__数据集长度 __getitem__根据索引取出数据集内元素

Dataloader：batch_size 处理数据的批次大小 shuffle 是否打乱数据（训练集通常为True）



## word2vec：用于生成词向量

- 用word2vec计算句子间相似度：

​	将句子分词、获取词分量、计算句子向量、计算句子向量之间的相似度

![image-20250702163105644](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702163105644.png)

#### 计算参数的方法：

1. **N-gram模型**：一个词出现的概率仅与它前面的n-1个词相关

![image-20250702163248508](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702163248508.png)

​	其主要工作是在语料中统计各种词串出现的次数以及平滑化处理。概率值计算好之后就存储起	来，下次需要计算一个句子的概率时，只需找到相关的概率参数，将它们连乘起来就好了。

2. **神经概率语言模型：**词语之间的相似性可以通过词向量来体现

   **词向量的编码方式**：One-Hot Representation 和 Distributed Representation。

   - One-Hot Representation：用一个很长的向量来表示一个词，向量的长度为词典的大小，向量中只有一个 1 ， 其他全为 0 ，1 的位置对应该词在词典中的位置。

   ![image-20250702163454895](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702163454895.png)

   ​	缺点：1.维数灾难 2.不能很好地刻画词与词之间的相似性 3.强稀疏性，0太多了不好计算

   - Distributed Representation：通过训练将每一个词映射成一个固定长度的短向量（“短”是相对于One-Hot Representation的“长”而言的），所有这些向量构成一个词向量空间，而每一个向量则可视为该空间中的一个点，就可以根据这些点的距离即词之间的距离来判断它们之间的语法、语义上的相似性了。

### word2vec：采用Distributed Representation

模型包括输入层、隐藏层和输出层，分为CBOW和skip-gram两种![image-20250702163921241](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702163921241.png)

#### CBOW

![image-20250702164010082](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164010082.png)

![image-20250702164016524](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164016524.png)

#### Skip-gram

![image-20250702164055116](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164055116.png)

## 两种加快word2vec训练速度的方法：Hierarchical softmax和Negative Sampling

#### 1. Hierarchical softmax



![image-20250702164343915](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164343915.png)

对CBOW:用随机梯度上升法来优化，使目标函数最大化![image-20250702164512311](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164512311.png)

对Skip-gram：![image-20250702164656413](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164656413.png)![image-20250702164716431](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702164716431.png)





#### 2. Negative Sampling

利用已知的概率密度函数来估计未知的概率密度函数。假设未知的概率密度函数为X，已知的概率密度为Y，如果得到了X和Y的关系，那么X也就可以求出来了的一个简化版。不再使用复杂的Huffman树，而是利用相对简单的随机负采样，能大幅度提高性能。

负采样算法：词典D中的词在语料C中出现的次数有高有底，对于那些高频词，被选为负样本的概率就应该比较大，反之，对于那些低频词，其被选中的概率就应该比较小。

![image-20250702165048199](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702165048199.png)

![image-20250702165052649](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250702165052649.png)

