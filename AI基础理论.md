# 基础理论

[TOC]

## *Transformer*

### 1.优缺点：

#### 优点

- 用注意力机制替代循环计算，实现序列并行处理，处理速度快
- 长距离记忆，而 LSTM 可能需要分阶段记忆，容易遗漏关联
- 首次证明仅靠注意力即可构建强大的序列模型
- 为后续 GPT、BERT 等大模型奠定架构基础

#### 缺点

- 计算量大，需要高端GPU支持
- 需要的语料大，训练一个基础transformer需要数亿单词语料，数据不足容易学偏
- 如果没有位置编码会缺失对语料里的顺序感知，会认为"小明吃苹果" 和 "苹果吃小明"相同

### 2.预测下一个词的底层逻辑：

Transformer 的文本生成本质是 "概率游戏"：

1. 输入句子 "今天天气"，模型分析每个词的关联
2. 计算 "今天" 与 "天气" 的关联度，判断 "天气" 后最可能跟 "晴朗"、"寒冷" 等词
3. 通过 softmax 函数输出每个候选词的概率，选概率最高的作为预测结果

### 3.Encoder：编码输入序列为上下文向量

![image-20250712135629034](AI基础理论.assets/image-20250712135629034.png)

####  1. 输入处理
- Token Embedding：将词语映射为向量![image-20250712151050966](AI基础理论.assets/image-20250712151050966.png)
- Positional Encoding：添加位置信息（正弦/余弦或学习型）

####  2. Encoder Layer（重复 N 层）
每层包含两个子层：
1. 多头自注意力 Multi-Head Self-Attention（具体见下方）：模仿词间依赖关系
2. 前馈网络 Feed-Forward Network：对每个位置的表示进行独立的两层前馈网络变换（ReLU 激活）。

####  3. Self-Attention 公式
对于每个 Attention Head：

```math
Attention(Q, K, V) = softmax(QK^T / √d_k) V
```

多个 Head 并行后进行拼接，再经过线性层：
$$
MultiHead(Q,K,V) = Concat(head₁, ..., head₈) * W^O
$$

### 4.Decoder

Decoder 每层由 3 个子层组成：

1. Masked Multi-Head Self-Attention：依赖于已生成的目标序列前缀，只能看到当前位置之前的token
2. Cross-Attention（与 Encoder 输出交互）：提供上下文信息
3. Feed-Forward Network：依赖于自身前层输出，用于提高表达能力

### 5.拓展

**BERT**：仅使用 Encoder → 表达句子语义

**GPT**：仅使用 Decoder → 自回归生成

**T5**：Encoder-Decoder 结构 → 多任务统一框架

------

------



## *Flash Attention*

Flash Attention是一种注意力机制优化技术，核心目标是解决标准自注意力机制在长序列处理时的内存瓶颈与计算低效问题

- 标准注意力计算流程：

```
HBM读取Q/K/V矩阵 → SRAM计算注意力分数 → HBM写入中间结果
```

该过程中数据在 HBM 与 SRAM 间的频繁搬运成为性能瓶颈，尤其当序列长度 n>2048 时，数据搬运耗时占比超 70%。

### 具体技术

- **分块计算（Tiling）技术**：

Flash Attention 采用棋盘式分块策略，将注意力矩阵划分为 B×B 大小的子块（典型 B=256）：

1. 行分块处理：将 Q 矩阵按行划分为多个块，每次仅加载一个 Q 块至 SRAM
2. 列并行计算：K/V 矩阵按列分块，与 Q 块进行矩阵乘法
3. 流水线调度：前一个 Q 块计算时，后一个 Q 块已开始数据加载

以 n=4096 为例，标准注意力需一次性加载 4096×4096 的矩阵至显存，而 Flash Attention 通过分块将显存占用降至 (4096×256)，实现 16 倍内存优化

- **重计算（Recomputation）策略**：用计算换显存

传统反向传播需存储前向计算的中间激活值，Flash Attention 通过动态重计算技术：

1. 前向传播时不存储注意力权重矩阵
2. 反向传播时根据 Q/K/V 重新计算梯度
3. 内存占用从 O (n²) 降至 O (n)，在 n=8192 时可减少 90% 显存占用

- **内存访问模式优化**

Flash Attention 通过以下策略减少数据搬运：

1. 合并访存：将 Q/K/V 的分散读取合并为连续内存访问

2. 缓存复用：同一 K/V 块为多个 Q 块服务时复用 SRAM 缓存

3. 异步调度：计算与数据搬运重叠执行，隐藏 IO 延迟

在 A100 显卡上，标准注意力的实际带宽利用率仅 30%，而 Flash Attention 通过优化可将带宽利用率提升至 75% 以上。

- **数值稳定性保障**

为解决分块计算中的数值精度问题，Flash Attention 采用：

1. 块级 softmax：对每个分块独立计算 softmax，避免跨块数值溢出
2. 累加归一化：分块计算的注意力值累加后再归一化
3. 混合精度支持：前向计算用 FP16，反向传播用 FP32 保证梯度精度

实验表明，在 n=2048 时 Flash Attention 与标准实现的困惑度差异小于 0.5%，确保训练精度不受影响。

------

------



## *RAG*：检索增强生成技术

随着 GPT 等预训练模型参数规模突破万亿，模型生成能力显著提升，但**训练数据截止性与知识更新滞后**问题日益凸显。RAG 通过**动态检索外部知识库**，使模型在推理时能获取最新信息，形成 "预训练模型 + 动态知识库" 的混合架构，从根本上解决传统 LM 的 "知识截止" 与 "事实幻觉" 问题。

简言之：将文本进行切片/分块后进行embedding，存储到向量数据库中（每一个切片对应一个长度相等的向量），将用户的输入也切片/分块后进行embedding，在向量数据库中找到与问题向量距离最近的几个向量，组合成输出![image-20250712164056322](AI基础理论.assets/image-20250712164056322.png)

**优点：**

- 灵活性高：可以随时更新知识库中的内容，让模型获取最新的信息。
- 扩展性强：不需要重新训练模型，只需要更新知识库，就能让模型回答新的问题。

**缺点：**

- 依赖检索：如果知识库中的信息不准确或不完整，模型的回答也会受影响。
- 实时性要求高：需要快速检索和整合知识库中的信息，对性能有一定要求。

**适用场景：**

- 智能客服：快速查找解决方案，回答用户的问题。
- 问答系统：结合知识库回答复杂的、需要背景知识的问题。
- 研究辅助：帮助研究人员快速查找相关文献或数据。

| 对比维度 | 长文本处理                       | 知识库                     | 微调                           |
| :------- | :------------------------------- | :------------------------- | :----------------------------- |
| 核心目标 | 理解和生成长篇内容               | 提供背景知识，增强回答能力 | 优化模型在特定任务或领域的表现 |
| 优点     | 连贯性强，适合复杂任务           | 灵活性高，可随时更新       | 性能提升，定制化强             |
| 缺点     | 资源消耗大，上下文限制           | 依赖检索，实时性要求高     | 需要标注数据，硬件要求高       |
| 适用场景 | 写作助手、阅读理解               | 智能客服、问答系统         | 专业领域、特定任务、风格定制   |
| 额外数据 | 不需要，但可能需要优化上下文长度 | 需要知识库数据             | 需要特定领域的标注数据         |
| 重新训练 | 不需要，但可能需要优化模型       | 不需要，只需更新知识库     | 需要对模型进行进一步训练       |
| 技术实现 | 扩大上下文窗口                   | 检索+生成（RAG）           | 调整模型参数                   |
| 数据依赖 | 无需额外数据                     | 依赖结构化知识库           | 需要大量标注数据               |
| 实时性   | 静态（依赖输入内容）             | 动态（知识库可随时更新）   | 静态（训练后固定）             |
| 资源消耗 | 高（长文本计算成本高）           | 中（需维护检索系统）       | 高（训练算力需求大）           |
| 灵活性   | 中（适合单次长内容分析）         | 高（可扩展多知识库）       | 低（需重新训练适应变化）       |

### 关键技术模块

#### (1) 检索模块技术实现

- **向量检索 (Embedding-based Retrieval)**：
    - 使用 SBERT、CLIP 等模型将查询与文档编码为高维向量
    - 通过余弦相似度或内积计算相关性，典型如 FAISS 索引
    - 支持分布式检索架构，处理 TB 级文档库
- **语义检索 (Semantic Retrieval)**：
    - 结合 BM25 词袋模型与向量检索的混合检索
    - 引入查询扩展技术，如同义词扩展、实体链接
    - 支持结构化数据检索，如知识图谱关联查询

#### (2) 文档重排序模块

- **重排序模型**：
    - 基于 BERT 的交叉编码器 (cross-encoder) 架构
    - 输入 (query, document) 对，输出相关性分数
    - 常用模型：ANCE、ColBERT、DPR
- **重排序策略**：
    - 级联检索 (cascade retrieval)：先粗筛后精排
    - 多阶段重排序：向量检索→BM25→重排序模型

#### (3) 上下文构建技术

- **文档切分策略**：
    - 滑动窗口切分：固定窗口大小 + 重叠率
    - 语义切分：基于段落、章节或语义单元切分
    - 智能切分：结合关键信息密度动态调整切分点
- **上下文融合技术**：
    - 简单拼接：[query][document1][document2]...
    - 分层融合：query 与各文档独立交互后聚合
    - 动态权重：根据文档相关性分配融合权重

#### (4) LLM 生成优化

- **提示工程 (Prompt Engineering)**：
    - 系统提示 (System Prompt)：指定回答格式、角色
    - 示例提示 (Example Prompt)：提供 Few-Shot 示例
    - 动态提示：根据检索结果生成自适应提示
- **生成控制技术**：
    - 温度参数调节：控制生成随机性
    - 最大长度限制：避免生成冗余内容
    - 重复惩罚：抑制重复内容生成

------

------



## *Agent*

AI tools：提供给AI Agent使用的工具

AI Agent：能够在模型、工具和最终用户之间传话的程序，如果模型返回的不是system prompt想要的格式就重试，直到返回为正确格式为止，但这样重试效率太低，于是产生了function calling

Function Calling(在ai agent和ai之间通信)：代替了system prompt，规定了模型调用tools和返回的格式

## *MCP*

规定了MCP Server和MCP Client如何通信

MCP Server:提供tools、文件、prompt等服务

MCP Client:调用tool的agent

## *Prompt*

system prompt:描述ai的角色、性格、背景知识、语气等

user prompt:用户输入

![image-20250710164600820](C:\Users\14775\AppData\Roaming\Typora\typora-user-images\image-20250710164600820.png)

